<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>Tricks Summary 2020</title>
  <meta name="description" content="In the blog, I record several problems that I met with during coding. I hope the content is helpful.">
  <meta name="author" content="leopardpan">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Tricks Summary 2020">
  <meta name="twitter:description" content="In the blog, I record several problems that I met with during coding. I hope the content is helpful.">

  <meta property="og:type" content="article">
  <meta property="og:title" content="Tricks Summary 2020">
  <meta property="og:description" content="In the blog, I record several problems that I met with during coding. I hope the content is helpful.">

  <link rel="icon" type="image/png" href="images/favicon.png" />
  <link href="/images/favicon.png" rel="shortcut icon" type="image/png">

  <link rel="stylesheet" href="/css/main.css">
  <!-- <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"> -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">

  <link rel="canonical" href="http://C0ldstudy.github.io/2020/09/Ticks_Summary_2020/">
  <link rel="alternate" type="application/rss+xml" title="Jason Xu" href="http://C0ldstudy.github.io/feed.xml">

  <!-- kramdown-math-katex latex sytax for markdown -->
  <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous"> -->
  <!-- <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script> -->


<!-- Á´ôÁÇπÁªüËÆ° -->
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<!-- google ÁªüËÆ° -->
   
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-118980599-1', 'auto');
      ga('send', 'pageview');
  </script>
  

</head>

  <body>
    <span class="mobile btn-mobile-menu">
      <div class="nav_container">
         <nav class="nav-menu-item" style = "float:right">
            <i class="nav-menu-item">
              <a href="/#blog" title="" class="blog-button">
                Homepage
              </a>
            </i>
            
                <i class="nav-menu-item">
                  <a href="/timeline" title="timeline" class="btn-mobile-menu__icon">
                      Timeline
                  </a>
                </i>
            
                <i class="nav-menu-item">
                  <a href="/tags" title="tags" class="btn-mobile-menu__icon">
                      Tag
                  </a>
                </i>
            
                <i class="nav-menu-item">
                  <a href="/photography" title="photography" class="btn-mobile-menu__icon">
                      Photography
                  </a>
                </i>
            
                <i class="nav-menu-item">
                  <a href="/about" title="about" class="btn-mobile-menu__icon">
                      About me
                  </a>
                </i>
            
          </nav>
      </div>
    </span>




    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">
        <!-- Â§¥ÂÉèÊïàÊûú-start -->
        <div class="ih-item circle effect right_to_left">
            <a href="/#blog" title="ÂâçÂæÄ Jason Xu ÁöÑ‰∏ªÈ°µ" class="blog-button">
                <div class="img"><img src="/images/avatar.jpg" alt="img"></div>
                <div class="info">
                    <div class="info-back">
                        <h2>
                            
                                C0ldstudy
                            
                        </h2>
                        <p>
                           
                                Machine Learning / Security
                            
                        </p>
                    </div>
                </div>
            </a>
        </div>
        <!-- Â§¥ÂÉèÊïàÊûú-end -->
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for Jason Xu" class="blog-button">Jason Xu</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">Personal Station</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">In the end, we only regret the chances we didn't take.</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        <p class="panel-cover__description">üá®üá≥Shanghai -> üá≠üá∞Hong Kong -> üá∫üá∏Irvine</p>
        


        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">Homepage</a></li>
                
                  <li class="navigation__item"><a href="/timeline" title="timeline">Timeline</a></li>
                
                  <li class="navigation__item"><a href="/tags" title="tags">Tag</a></li>
                
                  <li class="navigation__item"><a href="/photography" title="photography">Photography</a></li>
                
                  <li class="navigation__item"><a href="/about" title="about">About me</a></li>
                
              </ul>
            </nav>
          </div>
        </div>


        </div>
      </div>
    </div>

    
    <div class="panel-cover--overlay cover-clear"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            


<article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title">Tricks Summary 2020</h1>
    <div class="post-meta">
      <img src="/images/calendar.png" width="20px"/>
      <time datetime="2020-09-12 00:00:00 -0700" itemprop="datePublished" class="post-meta__date date">2020-09-12 | </time>
      <span id="busuanzi_container_page_pv" style='display:none'> Read: <span id="busuanzi_value_page_pv"></span> Times</span>

    </div>
  </header>

  <section class="post">

    <p>In the blog, I record several problems that I met with during coding. I hope the content is helpful.</p>

<h2 id="install-nvidia-driver-cuda-and-cudnn-on-ubuntu">Install Nvidia Driver CUDA and CUDNN on Ubuntu</h2>

<p>Official installation tutorial <a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#download">link</a>.</p>

<ol>
  <li>
    <p>Use Command Line: <a href="https://www.cyberciti.biz/faq/ubuntu-linux-install-nvidia-driver-latest-proprietary-driver/">Tutorial</a></p>
  </li>
  <li>
    <p>Search for PPA which can be used to install packages by <code class="language-plaintext highlighter-rouge">apt-get</code>: <a href="https://launchpad.net/~graphics-drivers/+archive/ubuntu/ppa">link</a></p>

    <p>Following the steps:<a href="https://askubuntu.com/questions/1077061/how-do-i-install-nvidia-and-cuda-drivers-into-ubuntu">tutorial</a></p>

    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># add ppa and find suitable nvidia-driver</span>
<span class="nb">sudo </span>add-apt-repository ppa:graphics-drivers/ppa
<span class="nb">sudo </span>apt-get update
apt-cache search nvidia-driver
<span class="c"># sudo apt-get install nvidia-driver-version</span>
<span class="nb">sudo </span>apt-get <span class="nb">install </span>nvidia-440
<span class="c"># another way to install nvidia driver</span>
ubuntu-drivers devices
<span class="nb">sudo </span>ubuntu-drivers autoinstall

<span class="c"># add keys for your specific ubuntu version, be careful about ubuntu1x04</span>
<span class="nb">sudo </span>apt-key adv <span class="nt">--fetch-keys</span>  http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub
<span class="c"># if error message:</span>
<span class="c"># using the command:</span>
<span class="c"># wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub</span>
<span class="c"># sudo apt-key add 7fa2af80.pub</span>
<span class="nb">sudo </span>bash <span class="nt">-c</span> <span class="s1">'echo "deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /" &gt; /etc/apt/sources.list.d/cuda.list'</span>

<span class="nb">sudo </span>bash <span class="nt">-c</span> <span class="s1">'echo "deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 /" &gt; /etc/apt/sources.list.d/cuda_learn.list'</span>

<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt <span class="nb">install </span>cuda-10-1
<span class="nb">sudo </span>apt <span class="nb">install </span>libcudnn7

<span class="c"># add the following codes to ~/.bashrc</span>
<span class="c"># set PATH for cuda installation</span>
<span class="k">if</span> <span class="o">[</span> <span class="nt">-d</span> <span class="s2">"/usr/local/cuda/bin/"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span>/usr/local/cuda/bin<span class="k">${</span><span class="nv">PATH</span>:+:<span class="k">${</span><span class="nv">PATH</span><span class="k">}}</span>
    <span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda/lib64<span class="k">${</span><span class="nv">LD_LIBRARY_PATH</span>:+:<span class="k">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="k">}}</span>
<span class="k">fi

</span>reboot <span class="nt">-i</span>

<span class="c"># check settings</span>
nvcc <span class="nt">--version</span>
/sbin/ldconfig <span class="nt">-N</span> <span class="nt">-v</span> <span class="si">$(</span><span class="nb">sed</span> <span class="s1">'s/:/ /'</span> <span class="o">&lt;&lt;&lt;</span> <span class="nv">$LD_LIBRARY_PATH</span><span class="si">)</span> 2&gt;/dev/null | <span class="nb">grep </span>libcudnn
nvidia-smi
</code></pre></div>    </div>
  </li>
  <li>
    <p>Reboot system</p>
  </li>
</ol>

<p>Then the NVIDIA driver will be installed smoothly.  After that, we should install CUDA.</p>

<p>Check CUDA version: <code class="language-plaintext highlighter-rouge">cat /usr/local/cuda/version.txt</code>.</p>

<p>If we want to download package by ourselves, we should check the <a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1604&amp;target_type=debnetwork">website</a> first to find the right version for our system. The greatest way to install it is following the <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">official guide</a>.</p>

<p>There is also a supportive <a href="https://mc.ai/easily-install-tensorflow-gpu-2-0-on-linux-ubuntu-18-04-cuda-10-cudnn-7-6-5/">tutorial</a> for installation cuda and cudnn for ubuntu.</p>

<h4 id="debian-cuda-install">Debian CUDA install</h4>

<p>Recently, I need to install cuda11-1 and nvidia driver on debian 11. Here is something that I achieve based on the experience.</p>

<p>The key point is that the cuda package can be used smoothly on debian 11 from my testing.</p>

<p>The faster method is using <code class="language-plaintext highlighter-rouge">sudo apt install nvidia-cuda-toolkit </code>. However, it can just install the latest version. In my case, it will install cuda11-2 while torch cannot support such a new version.</p>

<p>Then I tried several methods on the internet. But the solution is just based on the previous method.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>add-apt-repository ppa:graphics-drivers/ppa
<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt <span class="nb">install </span>nvidia-driver


<span class="nb">sudo </span>apt-key adv <span class="nt">--fetch-keys</span>  http://developer.download.nvidia.com/compute/cuda/repos/debian10/x86_64/7fa2af80.pub

<span class="nb">sudo </span>bash <span class="nt">-c</span> <span class="s1">'echo "deb http://developer.download.nvidia.com/compute/cuda/repos/debian10/x86_64 /" &gt; /etc/apt/sources.list.d/cuda.list'</span>

<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt <span class="nb">install </span>cuda-11-1

<span class="k">if</span> <span class="o">[</span> <span class="nt">-d</span> <span class="s2">"/usr/local/cuda/bin/"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span>/usr/local/cuda/bin<span class="k">${</span><span class="nv">PATH</span>:+:<span class="k">${</span><span class="nv">PATH</span><span class="k">}}</span>
    <span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda/lib64<span class="k">${</span><span class="nv">LD_LIBRARY_PATH</span>:+:<span class="k">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="k">}}</span>
<span class="k">fi

</span>reboot <span class="nt">-i</span>
</code></pre></div></div>

<p>Then just enjoy the new world.</p>

<h2 id="conda-usage">Conda Usage</h2>

<h4 id="1-search-available-python-version">1. search available python version</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conda</span> <span class="n">search</span> <span class="s">"^python$"</span>
</code></pre></div></div>

<h4 id="2-create-or-remove-conda-environment">2. Create or Remove conda environment</h4>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda remove <span class="nt">--name</span> myenv <span class="nt">--all</span>
conda create <span class="nt">-n</span> <span class="nb">env </span><span class="nv">python</span><span class="o">==</span>3.6
</code></pre></div></div>

<h2 id="tensorflow-usage-problem-recording">Tensorflow Usage Problem Recording</h2>

<h4 id="1-tensorflow-does-not-use-gpu">1. Tensorflow does not use GPU</h4>

<p>In the case, I actually installed cuda and Nvidia driver at first. So it is because I did not add <code class="language-plaintext highlighter-rouge">cuda/bin</code> and related library to the <code class="language-plaintext highlighter-rouge">.bashrc</code>.</p>

<p>By adding the following code to <code class="language-plaintext highlighter-rouge">.bashrc</code> file will solve the problem.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span>/usr/local/cuda-10.2/bin/:<span class="nv">$PATH</span>
<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda-10.2/lib64:<span class="k">${</span><span class="nv">LD_LIBRARY_PATH</span>:+:<span class="k">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="k">}}</span>
</code></pre></div></div>

<p>Besides, I think it is a useful <a href="https://mc.ai/tensorflow-gpu-installation-on-ubuntu-18-04/">tutorial</a> for use Tensorflow on GPU.</p>

<p><strong>Tensorflow2.1 is not useful for CUDA10.2 due to the lack of some libraries.</strong></p>

<p>So I reinstall CUDA10.1 for tensorflow2.1-gpu.</p>

<p>Test tensorflow for GPU</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="n">tf</span><span class="p">.</span><span class="n">test</span><span class="p">.</span><span class="n">is_gpu_available</span><span class="p">(</span><span class="n">cuda_only</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">min_cuda_compute_capability</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="2-install-tensorrt-for-tensorflow2-gpu-version">2. Install TensorRT for Tensorflow2-GPU version</h4>

<p>Official <a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.html">tutorial</a>: TensorRT is not a must for tensorflow-gpu. It is just useful for speed up training process.</p>

<h4 id="3-change-cuda-version">3. Change CUDA Version</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ll /usr/local/cuda
<span class="c"># make a link for cuda. usage: ln -s source target</span>
<span class="nb">ln</span> <span class="nt">-s</span> /usr/local/cuda-7.5 /usr/local/cuda
</code></pre></div></div>

<p><a href="https://stackoverflow.com/questions/45477133/how-to-change-cuda-version">Explanation</a></p>

<h4 id="4-cannot-visit-tensorfloworg-official-website">4. Cannot visit <code class="language-plaintext highlighter-rouge">tensorflow.org</code> official website</h4>

<p>In the case, the following step is useful for Mac. Linux should edit its specific <code class="language-plaintext highlighter-rouge">hosts</code> file.</p>

<ol>
  <li>edit /private/etc/hosts</li>
  <li>add <code class="language-plaintext highlighter-rouge">64.233.188.121 www.tensorflow.org</code></li>
</ol>

<h4 id="5-tensorflow-gpu-allocation-problem">5. Tensorflow GPU Allocation Problem</h4>

<p>By default, tensroflow will use all gpu memories to update efficiency. The way to use specific memory is by following codes in two ways. More details are available on the <a href="https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth">link</a></p>

<p><code class="language-plaintext highlighter-rouge">tf.config.experimental.set_memory_growth</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">)</span>
<span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="c1"># Currently, memory growth needs to be the same across GPUs
</span>    <span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
      <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
    <span class="n">logical_gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">list_logical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gpus</span><span class="p">),</span> <span class="s">"Physical GPUs,"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">logical_gpus</span><span class="p">),</span> <span class="s">"Logical GPUs"</span><span class="p">)</span>
  <span class="k">except</span> <span class="nb">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="c1"># Memory growth must be set before GPUs have been initialized
</span>    <span class="k">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">tf.config.experimental.set_virtual_device_configuration</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">)</span>
<span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
  <span class="c1"># Restrict TensorFlow to only allocate 1GB of memory on the first GPU
</span>  <span class="k">try</span><span class="p">:</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">set_virtual_device_configuration</span><span class="p">(</span>
        <span class="n">gpus</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">VirtualDeviceConfiguration</span><span class="p">(</span><span class="n">memory_limit</span><span class="o">=</span><span class="mi">1024</span><span class="p">)])</span>
    <span class="n">logical_gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">list_logical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gpus</span><span class="p">),</span> <span class="s">"Physical GPUs,"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">logical_gpus</span><span class="p">),</span> <span class="s">"Logical GPUs"</span><span class="p">)</span>
  <span class="k">except</span> <span class="nb">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="c1"># Virtual devices must be set before GPUs have been initialized
</span>    <span class="k">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="6-pytorch-jupyter-set-gpu">6. Pytorch Jupyter set GPU</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">())</span> <span class="c1"># list visible GPU
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda:6"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span> <span class="c1"># set GPU for device
</span><span class="n">model_ft</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model_ft</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># set GPU for models
</span></code></pre></div></div>

<h4 id="7-data-loader-problem">7. Data Loader Problem</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">raw_train_X</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))[</span><span class="mi">0</span><span class="p">].</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># (100000, 3, 64, 64)
</span><span class="n">raw_train_Y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))[</span><span class="mi">1</span><span class="p">].</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># (100000, )
</span></code></pre></div></div>

<p>The aforementioned code is not right because <code class="language-plaintext highlighter-rouge">next(iter(dataloader))</code> will re-output so the X and Y are not mapping.</p>

<h4 id="8-check-label-counts">8. Check label counts</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">unique</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">sy_train</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="9-allocation-problem">9. Allocation problem</h4>

<p>Warning: <code class="language-plaintext highlighter-rouge">ensorflow/core/framework/allocator.cc:101] Allocation of X exceeds 10% of system memory</code></p>

<p>Solution: The main problem is the <code class="language-plaintext highlighter-rouge">batch_size</code> is too big. Sometimes the problem is on related settings like <code class="language-plaintext highlighter-rouge">shuffle</code> function in<code class="language-plaintext highlighter-rouge">tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle()</code>.</p>

<h4 id="10-how-to-use-tffunction-need-digging-into">10. How to use <code class="language-plaintext highlighter-rouge">@tf.function</code> [Need Digging into]</h4>

<p>When I want to change the gradients when training by function <code class="language-plaintext highlighter-rouge">train_step</code> on tensorflow2, I found that if I did not use <code class="language-plaintext highlighter-rouge">@tf.function</code> before the <code class="language-plaintext highlighter-rouge">train_step</code> function, the accuracy grow very slow. But if I add <code class="language-plaintext highlighter-rouge">@tf.function</code>, the process becomes normal without considering optimizers and data type. The related code is here.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">tf</span><span class="p">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">fix_train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">all_mask</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_object</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
  <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">all_mask</span><span class="p">)):</span>
    <span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">all_mask</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
  <span class="n">optimizer</span><span class="p">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">))</span>
  <span class="n">train_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
  <span class="n">train_accuracy</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="11-problem-after-suspending-the-machine">11. Problem after suspending the Machine</h4>

<p>After I suspend the ubuntu, I met with such problem<code class="language-plaintext highlighter-rouge">failed call to cuInit: CUDA_ERROR_UNKNOWN</code> and I cannot use GPU. Rebooting it probably can solve the problem. As to my case, I reinstall nvidia-smi 440 solve the problem.</p>

<p>Related <a href="https://github.com/tensorflow/tensorflow/issues/394#">link</a></p>

<h4 id="12-save-model-problem-in-tensorflow2">12. Save Model Problem in Tensorflow2</h4>

<p>When I want to save a Keras subclass model, it will meet with the no bounded node error when I want to use tf.keras.models.Model to get middle layers‚Äô output. So in tensorflow 2, the suitable way to save and load model is listed as follows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># save model weights
</span><span class="n">model</span><span class="p">.</span><span class="n">save_weights</span><span class="p">(</span><span class="n">MODEL_FILEPATH</span> <span class="o">+</span> <span class="s">'weight.h5'</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">MODEL_FILEPATH</span> <span class="o">+</span> <span class="s">"weight.h5"</span><span class="p">)</span>

<span class="c1"># save whole model
</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">MODEL_FILEPATH</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">MODEL_FILEPATH</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="13-check-gpu-is-available-or-not-on-tensorflow">13. Check GPU is available or not on tensorflow:</h4>

<p><code class="language-plaintext highlighter-rouge">print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
tf.config.list_physical_devices('GPU')</code></p>

<p>The version compatible version of Tensorflow <a href="https://www.tensorflow.org/install/source#linux">details</a>.</p>

<h2 id="other-problems">Other Problems</h2>

<h4 id="1-install-cudnn-on-debian">1. Install Cudnn on Debian</h4>

<p>The installation steps of installing cudnn on debian is different from that of ubuntu. And the path of cuda related package is not the same of that of ubuntu.</p>

<p>Step 1: Download Cudnn on <a href="https://developer.nvidia.com/rdp/cudnn-download">Nvidia official website</a>.</p>

<p>Step 2: Add the related lib to the path <code class="language-plaintext highlighter-rouge">./usr/lib/x86_64-linux-gnu/</code>. A good way to find the path is by <code class="language-plaintext highlighter-rouge">find . -name libcublas.so.10</code>.</p>

<p>Step 3: Check the result of installation of cudnn.</p>

<h4 id="2-use-matplotlib-to-draw-3d-gradient-descent-pictures">2. Use matplotlib to draw 3D gradient descent pictures</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits</span> <span class="kn">import</span> <span class="n">mplot3d</span> <span class="c1">#Áî®‰∫éÁªòÂà∂3DÂõæÂΩ¢
</span>

<span class="c1">#Ê¢ØÂ∫¶ÂáΩÊï∞ÁöÑÂØºÊï∞
</span><span class="k">def</span> <span class="nf">gradJ1</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">4</span><span class="o">*</span><span class="n">theta</span>
<span class="k">def</span> <span class="nf">gradJ2</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">theta</span>

<span class="c1">#Ê¢ØÂ∫¶ÂáΩÊï∞
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span>  <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">ff</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span><span class="n">epoch</span><span class="p">,</span><span class="n">theta1</span><span class="p">,</span><span class="n">theta2</span><span class="p">,</span><span class="n">up</span><span class="p">,</span><span class="n">dirc</span><span class="p">):</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta1</span><span class="p">]</span>
    <span class="n">t2</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta2</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">gradJ1</span><span class="p">(</span><span class="n">theta1</span><span class="p">)</span>
        <span class="n">theta1</span> <span class="o">=</span> <span class="n">theta1</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gradient</span>
        <span class="n">t1</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta1</span><span class="p">)</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">gradJ2</span><span class="p">(</span><span class="n">theta2</span><span class="p">)</span>
        <span class="n">theta2</span> <span class="o">=</span> <span class="n">theta2</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gradient</span>
        <span class="n">t2</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta2</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>     <span class="c1">#ËÆæÁΩÆÁîªÂ∏ÉÂ§ßÂ∞è
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">,</span> <span class="n">ff</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span><span class="n">t2</span><span class="p">))</span>
<span class="c1">#     ax.scatter(t1, t2, ff(t1,t2), c='black',marker = '*', linewidth=1)
</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'none'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span> <span class="c1">#Êõ≤Èù¢Âõæ
</span>    <span class="c1">#ax.plot_wireframe(X, Y, Z, color='c') #Á∫øÊ°ÜÂõæ
#     ax.contour3D(X, Y, Z, 50, cmap='binary')#Á≠âÈ´òÁ∫øÂõæ
#     ax.scatter3D(t1, t2, ff(t1,t2), c='black',marker = 'o')
</span>    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">,</span> <span class="n">ff</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span><span class="n">t2</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span><span class="n">marker</span> <span class="o">=</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">#     ax.plot_wireframe(t1, t2, ff(t1,t2))
#     ax.plot3D(t1, t2,  ff(t1,t2),'red')
</span>    <span class="c1">#Ë∞ÉÊï¥ËßÇÂØüËßíÂ∫¶ÂíåÊñπ‰ΩçËßí„ÄÇËøôÈáåÂ∞Ü‰øØ‰ª∞ËßíËÆæ‰∏∫60Â∫¶ÔºåÊääÊñπ‰ΩçËßíË∞ÉÊï¥‰∏∫35Â∫¶
</span>    <span class="n">ax</span><span class="p">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">up</span><span class="p">,</span> <span class="n">dirc</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"./temp.png"</span><span class="p">)</span>

<span class="c1">#ÂèØ‰ª•ÈöèÊó∂Ë∞ÉËäÇÔºåÊü•ÁúãÊïàÊûú (ÊúÄÂ∞èÂÄºÔºåÊúÄÂ§ßÂÄºÔºåÊ≠•Èïø)
</span><span class="o">@</span><span class="n">interact</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.0002</span><span class="p">),</span><span class="n">epoch</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">init_theta1</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span><span class="n">init_theta2</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span><span class="n">up</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">180</span><span class="p">,</span><span class="mi">180</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">dirc</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">180</span><span class="p">,</span><span class="mi">180</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">continuous_update</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c1">#lr‰∏∫Â≠¶‰π†ÁéáÔºàÊ≠•ÈïøÔºâ epoch‰∏∫Ëø≠‰ª£Ê¨°Êï∞   init_theta‰∏∫ÂàùÂßãÂèÇÊï∞ÁöÑËÆæÁΩÆ upË∞ÉÊï¥ÂõæÁâá‰∏ä‰∏ãËßÜËßí dircË∞ÉÊï¥Â∑¶Âè≥ËßÜËßí
</span><span class="k">def</span> <span class="nf">visualize_gradient_descent</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span><span class="n">epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">init_theta1</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span><span class="n">init_theta2</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span><span class="n">up</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span><span class="n">dirc</span><span class="o">=</span><span class="mi">60</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span><span class="n">epoch</span><span class="p">,</span><span class="n">init_theta1</span><span class="p">,</span><span class="n">init_theta2</span><span class="p">,</span><span class="n">up</span><span class="p">,</span><span class="n">dirc</span><span class="p">)</span>

</code></pre></div></div>

<h4 id="3-git-add-new-repo">3. git add new repo</h4>

<p><a href="https://docs.github.com/en/github/importing-your-projects-to-github/adding-an-existing-project-to-github-using-the-command-line">link</a></p>

<h4 id="4-out-of-memory-on-pytorch">4. Out of memory on Pytorch</h4>

<p>When trainingÔºå memory usage of GPU will increase by calculating loss, output. So delete them when they are useless is a suitable way to decrease memory usage.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">del</span> <span class="n">cost</span><span class="p">,</span> <span class="n">out</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">all"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">memory_allocated</span><span class="p">())</span>
</code></pre></div></div>

<h4 id="5-mac-new-application-damaged-problem">5. Mac New Application Damaged Problem</h4>

<p><a href="https://qiujunya.com/article/2019/6/15/5.html">link</a></p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">sudo spctl --master-disable</code>
    <ul>
      <li>Enable it again: <code class="language-plaintext highlighter-rouge">sudo spctl --master-enable</code></li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">xattr -r -d com.apple.quarantine &lt;path&gt;</code>
    <ul>
      <li>` xattr -r -d com.apple.quarantine /Applications/PDF\ Expert.app`</li>
    </ul>
  </li>
</ol>

<h4 id="6-display-the-pictures-from-the-remote-ubuntu-on-the-local-server">6. Display the pictures from the remote ubuntu on the local server</h4>

<p><a href="https://stackoverflow.com/questions/3453188/matplotlib-display-plot-on-a-remote-machine">link</a></p>

<ol>
  <li>Install <code class="language-plaintext highlighter-rouge">xquartz</code> on mac from the <a href="https://www.xquartz.org/">link</a>.</li>
  <li>Use <code class="language-plaintext highlighter-rouge">ssh -X &lt;remote server&gt;</code> to enable x11 forwarding</li>
  <li>Run python script with <code class="language-plaintext highlighter-rouge">matplotlib</code> to build the connection.</li>
</ol>

<p>Enable OpenCL
Problems:</p>
<ol>
  <li><a href="https://www.scm.com/doc/Installation/Remote_GUI.html#opengl-direct-or-indirect-rendering">link</a></li>
</ol>

<pre><code class="language-Shell">libGL error: No matching fbConfigs or visuals found
libGL error: failed to load driver: swrast
</code></pre>

<p>set parameters:
<code class="language-plaintext highlighter-rouge">export LIBGL_ALWAYS_INDIRECT=1</code>
export LIBGL_DEBUG=verbose
export LIBGL_ALWAYS_SOFTWARE=1</p>

<ol>
  <li>Open3d Problem</li>
</ol>

<pre><code class="language-Shell">[Open3D WARNING] GLFW Error: GLX: Forward compatibility requested but GLX_ARB_create_context_profile is unavailable
[Open3D WARNING] Failed to create window
[Open3D WARNING] [DrawGeometries] Failed creating OpenGL window.
</code></pre>

<p>clinfo
glxinfo: OpenGL version string: 1.4 (2.1 INTEL-16.1.7) Need to be change to nvidia driver
command:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvidia-settings
<span class="nb">sudo </span>prime-select nvidia
</code></pre></div></div>
<p><a href="https://www.linuxbabe.com/desktop-linux/switch-intel-nvidia-graphics-card-ubuntu">link</a></p>

<p>opengl version with GPU
https://opengl.gpuinfo.org/displayreport.php?id=5738</p>

<h4 id="7-how-to-install-a-editable-python-package">7. How to install a editable python package</h4>

<p><a href="https://realpython.com/python-import/#create-and-install-a-local-package">Detailed Tutorial</a></p>

<p>Basically, <code class="language-plaintext highlighter-rouge">setup.cfg</code> and <code class="language-plaintext highlighter-rouge">setup.py</code> are configured for the editable package.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># setup.cfg
</span>
<span class="p">[</span><span class="n">metadata</span><span class="p">]</span>
<span class="n">name</span> <span class="o">=</span> <span class="n">local_structure</span>
<span class="n">version</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">.</span><span class="mi">0</span>

<span class="p">[</span><span class="n">options</span><span class="p">]</span>
<span class="n">packages</span> <span class="o">=</span> <span class="n">structure</span>

<span class="c1"># setup.py
</span>
<span class="kn">import</span> <span class="nn">setuptools</span>

<span class="n">setuptools</span><span class="p">.</span><span class="n">setup</span><span class="p">()</span>
</code></pre></div></div>

<p>And then use the following command to install the package locally under the package folder: <code class="language-plaintext highlighter-rouge">python -m pip install -e .</code></p>

<h4 id="8-the-information-about-the-file-system-and-the-cooresponding-operating-system">8. The Information about the file system and the cooresponding operating system</h4>

<p><strong>Linux</strong>:</p>

<ul>
  <li>
    <p>Best Recommands: Ext4</p>
  </li>
  <li>
    <p>Do not support: Exfat, Fat</p>
  </li>
</ul>

<p><strong>Mac</strong>:</p>

<ul>
  <li>
    <p>Do not support: Ext4</p>
  </li>
  <li>
    <p>Need kernel extension: <a href="https://github.com/osxfuse/osxfuse">NTFS</a></p>
  </li>
  <li>
    <p><a href="https://ggg.re/macos-mkfs-ext4">How to format a disk to Ext4</a></p>
  </li>
</ul>

<h4 id="9-the-related-things-about-external-disk-on-ubuntu">9. The Related things about external disk on Ubuntu</h4>

<p>When we mount a NTFS external disk on Ubuntu, all the files are owned by root and the priviledge is not able to be changed. That will leads to some priviledge problems when using softwares in it.</p>

<p>So the most suitable way is using an Ext4 external disk for Ubuntu.</p>

<h4 id="10-latex-one-table-covers-two-columns">10. latex one table covers two columns</h4>

<p><a href="https://tex.stackexchange.com/questions/3527/put-a-table-at-the-bottom-of-a-page">link</a></p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">\usepackage</span><span class="p">{</span>stfloats<span class="p">}</span>

<span class="nt">\begin{table*}</span>[tp]
    <span class="k">\centering</span>
    <span class="nt">\begin{tabular}</span><span class="p">{</span>c|ccc|ccc|ccc|ccc<span class="p">}</span>
    <span class="k">\hline</span>
    <span class="k">\multirow</span><span class="p">{</span>2<span class="p">}{</span>*<span class="p">}{</span>Case<span class="p">}</span> <span class="p">&amp;</span> <span class="k">\multicolumn</span><span class="p">{</span>3<span class="p">}{</span>c<span class="p">}{</span>PointNet++(Random)<span class="p">}</span> <span class="p">&amp;</span> <span class="k">\multicolumn</span><span class="p">{</span>3<span class="p">}{</span>c<span class="p">}{</span>PointNet++(<span class="k">\Mname</span>)<span class="p">}</span> <span class="p">&amp;</span> <span class="k">\multicolumn</span><span class="p">{</span>3<span class="p">}{</span>c<span class="p">}{</span>ResGCN-28(Random)<span class="p">}</span> <span class="p">&amp;</span> <span class="k">\multicolumn</span><span class="p">{</span>3<span class="p">}{</span>c<span class="p">}{</span>ResGCN-28(<span class="k">\Mname</span>)<span class="p">}</span> <span class="k">\\</span>

    <span class="p">&amp;</span> <span class="p">$</span><span class="nb">L</span><span class="p">_</span><span class="m">2</span><span class="p">$</span> <span class="p">&amp;</span> Acc <span class="p">&amp;</span> mIoU <span class="p">&amp;</span> <span class="p">$</span><span class="nb">L</span><span class="p">_</span><span class="m">2</span><span class="p">$</span> <span class="p">&amp;</span> Acc <span class="p">&amp;</span> mIoU  <span class="p">&amp;</span> <span class="p">$</span><span class="nb">L</span><span class="p">_</span><span class="m">2</span><span class="p">$</span> <span class="p">&amp;</span> Acc <span class="p">&amp;</span> mIoU  <span class="p">&amp;</span> <span class="p">$</span><span class="nb">L</span><span class="p">_</span><span class="m">2</span><span class="p">$</span> <span class="p">&amp;</span> Acc <span class="p">&amp;</span> mIoU  <span class="k">\\</span>
    <span class="k">\hline</span>
    <span class="k">\hline</span>
    Best    <span class="p">&amp;</span> <span class="k">\\</span>
    Average <span class="p">&amp;</span> <span class="k">\\</span>
    Worst   <span class="p">&amp;</span> <span class="k">\\</span>
    <span class="k">\hline</span>
    <span class="nt">\end{tabular}</span>
    <span class="k">\caption</span><span class="p">{</span>The results of the non-targeted attack.<span class="p">}</span>
    <span class="k">\label</span><span class="p">{</span>tab:nt-performance<span class="p">}</span>
<span class="nt">\end{table*}</span>
</code></pre></div></div>

<h4 id="11-oh-my-zsh-completions-commands-with-repeated-words">11. Oh-my-zsh completions commands with repeated words</h4>

<p><a href="https://github.com/ohmyzsh/ohmyzsh/issues/5157#issuecomment-226031519">link</a></p>

<h4 id="12-download-bilibili-video-automatically">12. Download Bilibili video automatically</h4>

<p><code class="language-plaintext highlighter-rouge">you-get -l https://www.bilibili.com/video/BV1U7411a7xG\?p\=20 --debug</code></p>

<p>Use you-get command. The syntax is listed as follows:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>usage: you-get <span class="o">[</span>OPTION]... URL...

A tiny downloader that scrapes the web

optional arguments:
  <span class="nt">-V</span>, <span class="nt">--version</span>         Print version and <span class="nb">exit</span>
  <span class="nt">-h</span>, <span class="nt">--help</span>            Print this <span class="nb">help </span>message and <span class="nb">exit

</span>Dry-run options:
  <span class="o">(</span>no actual downloading<span class="o">)</span>

  <span class="nt">-i</span>, <span class="nt">--info</span>            Print extracted information
  <span class="nt">-u</span>, <span class="nt">--url</span>             Print extracted information with URLs
  <span class="nt">--json</span>                Print extracted URLs <span class="k">in </span>JSON format

Download options:
  <span class="nt">-n</span>, <span class="nt">--no-merge</span>        Do not merge video parts
  <span class="nt">--no-caption</span>          Do not download captions <span class="o">(</span>subtitles, lyrics, danmaku, ...<span class="o">)</span>
  <span class="nt">-f</span>, <span class="nt">--force</span>           Force overwriting existing files
  <span class="nt">--skip-existing-file-size-check</span>
                        Skip existing file without checking file size
  <span class="nt">-F</span> STREAM_ID, <span class="nt">--format</span> STREAM_ID
                        Set video format to STREAM_ID
  <span class="nt">-O</span> FILE, <span class="nt">--output-filename</span> FILE
                        Set output filename
  <span class="nt">-o</span> DIR, <span class="nt">--output-dir</span> DIR
                        Set output directory
  <span class="nt">-p</span> PLAYER, <span class="nt">--player</span> PLAYER
                        Stream extracted URL to a PLAYER
  <span class="nt">-c</span> COOKIES_FILE, <span class="nt">--cookies</span> COOKIES_FILE
                        Load cookies.txt or cookies.sqlite
  <span class="nt">-t</span> SECONDS, <span class="nt">--timeout</span> SECONDS
                        Set socket <span class="nb">timeout</span>
  <span class="nt">-d</span>, <span class="nt">--debug</span>           Show traceback and other debug info
  <span class="nt">-I</span> FILE, <span class="nt">--input-file</span> FILE
                        Read non-playlist URLs from FILE
  <span class="nt">-P</span> PASSWORD, <span class="nt">--password</span> PASSWORD
                        Set video visit password to PASSWORD
  <span class="nt">-l</span>, <span class="nt">--playlist</span>        Prefer to download a playlist
  <span class="nt">-a</span>, <span class="nt">--auto-rename</span>     Auto rename same name different files
  <span class="nt">-k</span>, <span class="nt">--insecure</span>        ignore ssl errors

Proxy options:
  <span class="nt">-x</span> HOST:PORT, <span class="nt">--http-proxy</span> HOST:PORT
                        Use an HTTP proxy <span class="k">for </span>downloading
  <span class="nt">-y</span> HOST:PORT, <span class="nt">--extractor-proxy</span> HOST:PORT
                        Use an HTTP proxy <span class="k">for </span>extracting only
  <span class="nt">--no-proxy</span>            Never use a proxy
  <span class="nt">-s</span> HOST:PORT, <span class="nt">--socks-proxy</span> HOST:PORT
                        Use an SOCKS5 proxy <span class="k">for </span>downloading
</code></pre></div></div>

<h4 id="13-recovery-the-deleted-files-on-ubuntu">13. Recovery the deleted files on Ubuntu</h4>

<p><a href="https://vitux.com/how-to-recover-deleted-files-in-ubuntu-through-testdisk/">testdisk</a></p>

<h4 id="14-how-to-set-the-local-mac-to-proxy-the-ubuntu-servers-packages">14. How to set the local Mac to proxy the Ubuntu server‚Äôs packages</h4>

<p>Step 1: Open Mac‚Äôs SSH service and Farword configure.</p>

<p>Step 2: Set Ubuntu proxy config</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># add the following two lines on the ~/.bashrc</span>
<span class="nb">export </span><span class="nv">https_proxy</span><span class="o">=</span>127.0.0.1:1234
<span class="nb">export </span><span class="nv">http_proxy</span><span class="o">=</span>127.0.0.1:1234

<span class="c"># set the ssh channel, 7890 is the VPN proxy port</span>
ssh <span class="nt">-N</span> <span class="nt">-f</span> <span class="nt">-L</span> localhost:1234:localhost:7890 jason@10.177.74.47&lt;<span class="nb">local </span>machine ssh service&gt;

<span class="c"># check the port-using process</span>
lsof <span class="nt">-ti</span>:1234

<span class="c"># check the vpn service</span>
curl <span class="nt">-I</span> https://google.com
</code></pre></div></div>

<h4 id="15-a-fast-way-to-transfer-files-between-remote-servers-with-progress-bar">15. A fast way to transfer files between remote servers with progress bar</h4>

<p><a href="https://www.cyberciti.biz/faq/show-progress-during-file-transfer/"><strong>rsync</strong></a></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rsync <span class="nt">-r</span> <span class="nt">--info</span><span class="o">=</span>progress2 &lt;files path&gt; &lt;username@remote server&gt; 
</code></pre></div></div>

<h4 id="16-jupyter-cannot-use-the-specific-environment-of-conda">16. Jupyter cannot use the specific environment of conda</h4>

<p>A helpful <a href="https://ipython.readthedocs.io/en/stable/install/kernel_install.html#kernels-for-different-environments">link</a>.</p>

<p>Basically, the main problem is that the system does not use the <code class="language-plaintext highlighter-rouge">jupyter</code> command in the conda environment. Instead, it uses the system default version.</p>

<p>We can use the <code class="language-plaintext highlighter-rouge">sys.path</code> to check whether we use the correct command or not.</p>

<p>If the result of <code class="language-plaintext highlighter-rouge">sys.path</code> is like the following, the environment is correct.</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="err">'/home/jxu/random-fourier-features/examples'</span><span class="p">,</span><span class="w">
 </span><span class="err">'/home/jxu/miniconda</span><span class="mi">3</span><span class="err">/envs/rf/lib/python</span><span class="mi">37</span><span class="err">.zip'</span><span class="p">,</span><span class="w">
 </span><span class="err">'/home/jxu/miniconda</span><span class="mi">3</span><span class="err">/envs/rf/lib/python</span><span class="mf">3.7</span><span class="err">'</span><span class="p">,</span><span class="w">
 </span><span class="err">'/home/jxu/miniconda</span><span class="mi">3</span><span class="err">/envs/rf/lib/python</span><span class="mf">3.7</span><span class="err">/lib-dynload'</span><span class="p">,</span><span class="w">
 </span><span class="err">''</span><span class="p">,</span><span class="w">
 </span><span class="err">'/home/jxu/miniconda</span><span class="mi">3</span><span class="err">/envs/rf/lib/python</span><span class="mf">3.7</span><span class="err">/site-packages'</span><span class="p">,</span><span class="w">
 </span><span class="err">'/home/jxu/miniconda</span><span class="mi">3</span><span class="err">/envs/rf/lib/python</span><span class="mf">3.7</span><span class="err">/site-packages/IPython/extensions'</span><span class="p">,</span><span class="w">
 </span><span class="err">'/home/jxu/.ipython'</span><span class="p">]</span><span class="w">
</span></code></pre></div></div>

<p>Basically, I did not exploit the detail of the problem this time. But I will list the solution here.</p>

<p><del>I install the jupyterhub by the command: <code class="language-plaintext highlighter-rouge">conda install -c conda-forge jupyterhub </code> from the <a href="https://stackoverflow.com/questions/43327052/how-to-uninstall-jupyter-notebook-installed-from-anaconda/43525766">link</a>.</del></p>

<p>I reinstall it by the commnads from the <a href="https://towardsdatascience.com/how-to-set-up-anaconda-and-jupyter-notebook-the-right-way-de3b7623ea4a">link</a>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge jupyterlab
conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge nb_conda_kernels
<span class="c"># conda install -c conda-forge jupyter_contrib_nbextensions</span>

</code></pre></div></div>

<p>Some userful jupyter extensions can be found <a href="https://towardsdatascience.com/jupyter-notebook-extensions-517fa69d2231">here</a>.</p>

<h4 id="17-use-slack-to-receive-signals-or-messages-from-the-commands">17. Use slack to receive signals or messages from the commands</h4>

<p>A helpful <a href="https://uci-dsp-lab.slack.com/services/B028Q57836W?settings=1&amp;utm_source=in-prod&amp;utm_medium=inprod-link_app_settings-user_card-click#service_setup">link</a>.</p>

<p>Command for the direct message to the user:</p>

<p><code class="language-plaintext highlighter-rouge">curl -X POST --data-urlencode "payload={\"channel\": \"@memberid\", \"username\": \"webhookbot\", \"text\": \"The machine with GTX1080 has been rebooted:)\", \"icon_emoji\": \":ghost:\"}" &lt;link&gt;</code></p>

<p>Command for the channel message:</p>

<p><code class="language-plaintext highlighter-rouge">curl -X POST --data-urlencode "payload={\"channel\": \"#general\", \"username\": \"webhookbot\", \"text\": \"The machine with GTX1080 has been rebooted:)\", \"icon_emoji\": \":ghost:\"}" &lt;link&gt;</code></p>

<p>For example, we can send a message to the user if the machine reboot.</p>

<p>We can write the command to the file <code class="language-plaintext highlighter-rouge">\etc\rc.local</code>.</p>

<h4 id="18-ubuntu-set-default-desktop">18. Ubuntu set Default Desktop</h4>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>update-alternatives <span class="nt">--config</span> x-session-manager
<span class="nb">sudo </span>dpkg-reconfigure gdm3 <span class="c"># set the default desktop</span>
</code></pre></div></div>

<p>Currently, I test it on Ubuntu 16.04 and figure out gdm3 cannot be run while lightdm works well. I am not sure the reason.</p>

  </section>
<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> -->
</article>

<section>
       <ul class="pager">
        
        <li class="previous">
            <a href="/2020/04/VS_Code_Plugins/" data-toggle="tooltip" data-placement="top" title="Visual Studio Code Plugins">Last BlogÔºö  <span>Visual Studio Code Plugins</span>
            </a>
        </li>
        
        
        <li class="next">
            <a href="/2020/10/Machine-Learning_Tutorial/" data-toggle="tooltip" data-placement="top" title="Machine Learning Tutorial">Next BlogÔºö  <span>Machine Learning Tutorial</span>
            </a>
        </li>
        
    </ul>
</section>

<section class="post-comments">
<div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      // Here is an example,
      this.page.url = "https://c0ldstudy.github.io/2020/09/Ticks_Summary_2020/";
      this.page.identifier = "/2020/09/Ticks_Summary_2020/";
    };

    // You should be able to get the following lines of code from your Disqus admin.
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = '//c0ldstudy-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
</section>


            <section class="footer">
    <footer>
        <div class = "footer_div">
          <div clas="recordings" style="display:flex;justify-content:space-around;">
          <div class="twitter_display">
            <div class='jekyll-twitter-plugin'><a class="twitter-timeline" data-width="300" data-tweet-limit="1" href="https://twitter.com/JiacenXu?ref_src=twsrc%5Etfw">Tweets by JiacenXu</a>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
          </div>
          <div class="github_chart">
            <img src="http://ghchart.rshah.org/409ba5/C0ldstudy" alt="C0ldstudy's Blue Github chart"/>
            <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=n&d=PWaejm3rz2TG6HOtwcekX2kM9kPWz2aP6GYhuOVoNpA&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
          </div>
        </div>
            <nav class="cover-navigation navigation--social">
            <ul class="navigation">
          
          <!-- Github -->
          <li class="navigation__item_social">
            <a href="https://github.com/c0ldstudy" title="@c0ldstudy ÁöÑ Github" target="_blank">
              <i class='fab fa-github fa-2x'></i>
              <span class="label">Github</span>
            </a>
          </li>
          
          
          <!-- Twitter -->
          <li class="navigation__item_social">
            <a href="https://twitter.com/JiacenXu" title="@JiacenXu" target="_blank">
              <i class='fab fa-twitter fa-2x'></i>
              <span class="label">Twitter</span>
            </a>
          </li>
          

          
          <!-- LinkedIn -->
          <li class="navigation__item_social">
            <a href="https://www.linkedin.com/in/jiacen-xu-021536105/" title="@jiacen-xu-021536105" target="_blank">
              <i class='fab fa-linkedin fa-2x'></i>
              <span class="label">LinkedIn</span>
            </a>
          </li>
          
          

          <!-- RSS -->
          <li class="navigation__item_social">
            <a href="/feed.xml" rel="author" title="RSS" target="_blank">
              <i class='fa fa-rss fa-2x'></i>
              <span class="label">RSS</span>
            </a>
          </li>

          
          <!-- Email -->
          <li class="navigation__item_social">
            <a href="mailto:coldstudyxu@gmail.com" title="Contact me">
              <i class='fa fa-envelope fa-2x'></i>
              <span class="label">Email</span>
            </a>
          </li>
          
          </ul>
        </nav>
        </div>

        <div class = "footer_div_counter">
           <p class="copyright text-muted">
            Copyright &copy; Jason Xu 2022 Theme by <a href="http://c0ldstudy.github.io/">C0ldstudy</a> |
            <iframe
                style="margin-left: 2px; margin-bottom:-5px;"
                frameborder="0" scrolling="0" width="91px" height="20px"
                src="https://ghbtns.com/github-btn.html?user=c0ldstudy&repo=c0ldstudy.github.io&type=star&count=true" >
            </iframe>
            </p>
        	<div align="right">
    			<!-- <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css"> -->
          <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">

          <!-- ËÆøÈóÆÁªüËÆ° -->
          <span id="busuanzi_container_site_uv" style='display:none'>
          <span id="busuanzi_value_site_uv"></span> People Visited!
          </span>
        </div>
        </div>
    </footer>
</section>

        </div>
    </div>

    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>
<script type="text/javascript" src="/js/main.js"></script>

<script type="text/javascript" src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>




  </body>

</html>
